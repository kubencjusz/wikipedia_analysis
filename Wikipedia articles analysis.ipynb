{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis in this notebook was conducted using Wikipedia dump (Polish language version) dated 01.04.2018 (this version isn't available online anymore, but you can download the latest dumps from https://dumps.wikimedia.org/plwiki/ ). The files prepared originally in XML format were parsed to JSON files using https://github.com/attardi/wikiextractor scripts in order to process them easier in Python.\n",
    "\n",
    "From all the articles written in Polish only those containing biographies were chosen (using the filtering condition that the phrase \"ur.\" or \"(ur.\" (shortcut for \"born\" in Polish) must be present in the first 500 characters of an article). After applying that condition almost 270k articles were selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WikipediaUtils import PrepareWikipedia # custom class to pre-process data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression # to check Zipf law\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob as tb # for alternative tf-idf implementation\n",
    "\n",
    "# stemming for Polish:\n",
    "from pyMorfologik import Morfologik\n",
    "from pyMorfologik.parsing import ListParser #, DictParser\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import operator\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common and at the same time the least useful terms are the so called ___stop words___. These are the words which in conjunction with other words form sentences or phrases, such as \"and\", \"but\", \"which\" etc. This kind of words should be removed from data as they introduce noise and are irrelevant from the perspective of interesting patterns in data. Another benefit of removing such words is less memory usage.\n",
    "\n",
    "The next step in the analysis was to lowercase all words and use a transformation called ___lemmatization___. Lemmatization is a process of creating a lemma i.e. the canonical, dicionary form of a word. For example as a result of lemmatizing the word \"went\" we would get word \"go\", or \"the best\"$\\rightarrow$\"good\", \"mice\"$\\rightarrow$\"mouse\". Other useful transformation is ___stemming___, which aims at finding a root form of a word. The result of stemming may not be a valid word which could be found in a dictionary.\n",
    "\n",
    "In order to stem articles written in Polish I used PyMorfologik package which is Polish morphological analyzer, which uses data from Narodowy Korpus Języka Polskiego (NKJP).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from WikipediaUtils import PrepareWikipedia\n",
    "wiki_obj = PrepareWikipedia(path=\"/home/ec2-user\", word=[\"ur.\", \"(ur.\"])\n",
    "\n",
    "biogr = wiki_obj.extract_articles()\n",
    "biogr = wiki_obj.remove_stop_words(biogr)\n",
    "biogr = wiki_obj.stem_articles(biogr)\n",
    "#final = wiki_obj.extract_text(biogr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf law verification\n",
    "Zipf law describes an empirically observed rule which in case of text analysis relates to the fact, that for a given set of words occuring within a corpus of documents, the frequency of a given word is inversely proportional to its rank in a frequency table calculated for the entire corpus. In other words, the most frequent word in a corpus should be twice as frequent as the second most frequent, three times as frequent as the third most frequent etc.\n",
    "\n",
    "$frequency_{word} = \\frac{const.}{rank_{word}}$\n",
    "\n",
    "The differences between theoretical and empirical distribution were verified for the analysed articles. The linear model was fitted for words with ranks ranging from 10 to 1000 in a frequency table (both explanatory and dependent variables were log-transformed). The plot shows that Zipf law doesn't apply in case of this corpus - the most frequent words occur less often than it would result from theoretical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of occurences for all words\n",
    "all_words = [x for y in wiki_obj.extract_text(biogr) for x in y.split(\" \")]\n",
    "word_counts = Counter(all_words)\n",
    "# sort the result\n",
    "word_counts_sorted = sorted(word_counts.items(), key=operator.itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_df = pd.DataFrame(word_counts_sorted, columns = ['word', 'n'])\n",
    "freq_df['fraction'] = freq_df['n'] / sum(freq_df['n'])\n",
    "freq_df['rank'] = range(1,(freq_df.shape[0]+1))\n",
    "\n",
    "freq_df_top = freq_df.loc[(freq_df['rank']<=1000) & (freq_df['rank']>=10), :]\n",
    "\n",
    "# fitting linear regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X=np.log10(freq_df_top[['rank']]), y=np.log10(freq_df_top['fraction']))\n",
    "slope = np.float(lin_reg.coef_)\n",
    "intercept = np.float(lin_reg.intercept_)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "# empirical:\n",
    "ax.plot(np.log10(freq_df_top['rank']), np.log10(freq_df_top['fraction']), color='blue', lw=2)\n",
    "# theoretical:\n",
    "ax.plot(np.log10(freq_df_top['rank']), slope*np.log10(freq_df_top['rank'])+intercept, color='red')\n",
    "ax.set(xlabel=\" log Rank\", ylabel=\" log Fraction\", title=\"Verification of Zipf law\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf calculation\n",
    "It is also useful to check the so called ___tf-idf___ (_term frequency - inverse document frequency_) statistics. It shows the weight, importance of a word in a particuar document taking into account the frequency of that word in a set of analysed documents.\n",
    "The statistics for a word $i$ in a document $d$ id calculated as a product of a frequency of that word in a document (term frequency part) and the logarithm of inverse frequency of that word in a corpus of documents:\n",
    "\n",
    "$tf(i, d) = \\frac{\\text{number of times words i occurs in document d}}{\\text{number of words in document d}}$\n",
    "\n",
    "$idf(i) = ln(\\frac{\\text{number of documents in a corpus}}{\\text{number of documents containing word i}})$\n",
    "\n",
    "In other words the tf-idf statistics is:\n",
    "* the largest when word $i$ occurs many times in a relatively small set of documents (including document $d$)\n",
    "* average when the word occurs rarely in document $d$ or appears in many documents in a corpus\n",
    "* the lowest when the word occurs in nearly all documents (_stop-words_ belong to this group)\n",
    "\n",
    "The following analysis was conducted for the top 100 (in terms of articles' length) biographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_lengths = []\n",
    "for indeks, art in enumerate(biogr):\n",
    "    krotka = (art['title'],len(art['text']))\n",
    "    art_lengths.append(krotka)\n",
    "    \n",
    "longest_art = pd.DataFrame(art_lengths, columns = ['title','length']).sort_values('length', ascending=False)\n",
    "\n",
    "titles = longest_art['title'].iloc[:100].tolist()\n",
    "\n",
    "tf_idf_art = []\n",
    "for art in biogr:\n",
    "    if art['title'] in titles:\n",
    "        tf_idf_art.append(art)\n",
    "\n",
    "tf_idf_art = sorted(tf_idf_art, key=lambda k: len(k['text']), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tword: opon, TF-IDF: 0.0022\n",
      "\tword: okrążenia, TF-IDF: 0.0022\n",
      "\tword: rajdzie, TF-IDF: 0.0022\n",
      "\tword: okrążeń, TF-IDF: 0.0022\n",
      "\tword: opony, TF-IDF: 0.0022\n",
      "Top words in document 2\n",
      "\tword: hawana, TF-IDF: 0.003\n",
      "\tword: fidel, TF-IDF: 0.0029\n",
      "\tword: kuba, TF-IDF: 0.0028\n",
      "\tword: batisty, TF-IDF: 0.0025\n",
      "\tword: fidela, TF-IDF: 0.0025\n",
      "Top words in document 3\n",
      "\tword: antoniusza, TF-IDF: 0.0027\n",
      "\tword: antoniusz, TF-IDF: 0.0027\n",
      "\tword: konsul, TF-IDF: 0.0026\n",
      "\tword: mówca, TF-IDF: 0.0025\n",
      "\tword: filipika, TF-IDF: 0.0024\n",
      "Top words in document 4\n",
      "\tword: ruś, TF-IDF: 0.003\n",
      "\tword: darwina, TF-IDF: 0.0027\n",
      "\tword: płocki, TF-IDF: 0.0025\n",
      "\tword: luksemburski, TF-IDF: 0.0024\n",
      "\tword: beagle, TF-IDF: 0.0022\n",
      "Top words in document 5\n",
      "\tword: eserowiec, TF-IDF: 0.0032\n",
      "\tword: mienszewik, TF-IDF: 0.003\n",
      "\tword: trocki, TF-IDF: 0.0029\n",
      "\tword: uljanow, TF-IDF: 0.0025\n",
      "\tword: sdprr, TF-IDF: 0.0024\n",
      "Top words in document 6\n",
      "\tword: 5:3, TF-IDF: 0.0031\n",
      "\tword: wbił, TF-IDF: 0.0029\n",
      "\tword: sesję, TF-IDF: 0.0028\n",
      "\tword: maksymalnego, TF-IDF: 0.0027\n",
      "\tword: anglika, TF-IDF: 0.0027\n",
      "Top words in document 7\n",
      "\tword: osa, TF-IDF: 0.0033\n",
      "\tword: indoeuropejski, TF-IDF: 0.003\n",
      "\tword: miękki, TF-IDF: 0.0027\n",
      "\tword: lokat, TF-IDF: 0.0023\n",
      "\tword: mieszanka, TF-IDF: 0.0023\n",
      "Top words in document 8\n",
      "\tword: deportacja, TF-IDF: 0.0034\n",
      "\tword: getto, TF-IDF: 0.0034\n",
      "\tword: auschwitz, TF-IDF: 0.0032\n",
      "\tword: eichmannowi, TF-IDF: 0.0031\n",
      "\tword: rsha, TF-IDF: 0.0029\n",
      "Top words in document 9\n",
      "\tword: kochanowskiego, TF-IDF: 0.0037\n",
      "\tword: kochanowski, TF-IDF: 0.0029\n",
      "\tword: naczelnik, TF-IDF: 0.0028\n",
      "\tword: daszyński, TF-IDF: 0.0028\n",
      "\tword: belweder, TF-IDF: 0.0024\n",
      "Top words in document 10\n",
      "\tword: chadek, TF-IDF: 0.0039\n",
      "\tword: miedź, TF-IDF: 0.0033\n",
      "\tword: prawica, TF-IDF: 0.0032\n",
      "\tword: puczysta, TF-IDF: 0.003\n",
      "\tword: freia, TF-IDF: 0.0029\n"
     ]
    }
   ],
   "source": [
    "# creating blobs of text\n",
    "doc_list = [tb(tf_idf_art[i]['text']) for i in range(len(tf_idf_art))]\n",
    "\n",
    "# creating counts for idf part\n",
    "unique_words = [x.words for x in doc_list]\n",
    "unique_words = [x for y in unique_words for x in y]\n",
    "idf_dict = dict(Counter(unique_words))\n",
    "\n",
    "# defining functions to calculate tf-idf statistics\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def idf(word, doc_list):\n",
    "    return np.log(len(doc_list) / (1 + idf_dict.get(word, 0)))\n",
    "\n",
    "def tfidf(word, blob, doc_list):\n",
    "    return tf(word, blob) * idf(word, doc_list)\n",
    "\n",
    "# showing most relevant words for the 10 longest articles\n",
    "for i, blob in enumerate(doc_list[:10]):\n",
    "    print(\"Top words in document {}\".format(i+1)) # tf_idf_art[i]['title']\n",
    "    scores = {word: tfidf(word, blob, doc_list) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:5]:\n",
    "        print(\"\\tword: {}, TF-IDF: {}\".format(word, round(score, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn package has a dedicated class to calculate tf-idf statistics. The table shows most important in terms of tf-idf statistics words for the longest articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['miejscu' 'prix' 'odcinku' ... 'klasyfikacji' 'okrążeniu' 'kubica']\n",
      " ['ruch' 'przywódca' 'batisty' ... 'kubański' 'kub' 'castra']\n",
      " ['republika' 'wiek' 'retoryczny' ... 'senat' 'cezar' 'cyceron']\n",
      " ...\n",
      " ['kuomintangu' 'rewolucji' 'hongkongu' ... 'jat' 'suna' 'sun']\n",
      " ['kraj' 'lomé' 'ufc' ... 'prezydent' 'toga' 'gnassingbé']\n",
      " ['open' 'il' 'zagrać' ... 'runda' 'chilijczyk' 'gonzález']]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_art = wiki_obj.extract_text(tf_idf_art)\n",
    "\n",
    "tfidf_obj = TfidfVectorizer(use_idf=True)\n",
    "X = tfidf_obj.fit_transform(tf_idf_art)\n",
    "#print(X)\n",
    "# tfidf_obj.get_params()\n",
    "\n",
    "feature_array = np.array(tfidf_obj.get_feature_names())\n",
    "tfidf_sorting = np.argsort(X.toarray(), axis = 1)#.flatten()[::-1]\n",
    "n = 15\n",
    "top_n = feature_array[tfidf_sorting][:, -n:]\n",
    "print(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation is a generative statistical model, which enables modeling of latent variables. It is widely used in a field called topic modeling. In that case observable variables are the words in analysed documents and unobservable (latent) variables are the topics which particular words and documents belong to.\n",
    "\n",
    "The generative process entails that each document is a composition of some latent variables (topics) and each topic is explained by probability distribution of all the words occuring in a corpus of documents. The following assumptions are considered:\n",
    "1. Each latent topic is described by the multinomial distribution of all words in a dictionary\n",
    "2. Each document is described by the Dirichlet distribution of topics\n",
    "3. Each word in a document is sampled from the distribution of topics\n",
    "\n",
    "LDA generative process for a single document looks like this:\n",
    "1. The number of words that constitute a document is chosen\n",
    "2. The distribution of topics for a document is sampled from the Dirichlet distribution (e.g. given we have 3 latent topics - 20% of words come from topic A, 30% from topic B and 50% from topic C)\n",
    "3. For each word the following process takes place (we assume that order of words is irrelevant):\n",
    "\t* Choosing a topic which the word comes from by sampling from Dirichlet distribution specified in preceding step\n",
    "\t* Choosing a word by sampling from multinomial distribution of words for the chosen topic\n",
    "\n",
    "\n",
    "\n",
    "![LDA diagram](./images/lda_diagram.png)\n",
    "\n",
    "\n",
    "The above diagram presents LDA model in a plate notation (a notation which makes it possible to represent random variables that recur in a model). $D$ is a set of documents in a corpus, $N$ is a particular document in a set of all documents, $K$ signifies all latent topics and $V$ is a set of all words in a dictionary. The parameters $\\alpha$ and $\\beta$ are vectors (of length $K$ and $V$ respectively) and describe a priori Dirichlet distribution of topics in document and words in topics respectively.\n",
    "\n",
    "$\\beta_k$ is a distribution of words occuring in topic $k$, and $\\theta_d$ is a probability distribution of topics occuring in document $d$. $Z_{d,n}$ is a topic assigned to word $n$ in a document $d$ sampled from distribution $\\theta_d$. $W_{d,n}$ is an observable $n$-th word in a document $d$. This word is dependent on a distribution of topics in a document $d$ - $\\theta_d$ and a distribution of words in topic $k$ ($\\beta_k$) which was sampled from distribution $\\theta_d$.\n",
    "\n",
    "Assuming all the variables are observable, the joint probability distribution resulting from the model is specified as:\n",
    "\n",
    "${\\displaystyle \\prod_{k=1}^{K} p(\\beta_{k}|\\alpha)} {\\displaystyle \\prod_{d=1}^{D} p(\\theta_{d}|\\eta)} {\\displaystyle \\prod_{n=1}^{N} p(Z_{d,n}|\\theta_d)p(W_{d,n}|Z_{d,n},\\beta_{1,...,K})}$\n",
    "\n",
    "Inference is conducted by using Gibbs sampling for each of the topics $k$ in accordance with the equation:\n",
    "\n",
    "$\\LARGE \\frac{n_{d,k}+\\alpha_k}{\\sum_{i}^{K} n_{d,i}+\\alpha_i}\\frac{v_{k,w_{d,n}}+\\lambda_{w_{d,n}}}{\\sum_{i}^{V} v_{k,i}+\\lambda_i}$, where:\n",
    "\n",
    "* ${\\displaystyle n_{d,k}}$: how many times a document $d$ assigned topic $k$ to its words\n",
    "* $v_{k,w_{d,n}}$: how many times a word $w_{d,n}$ was chosen by topic $k$\n",
    "* $\\alpha_k$: the parameter of Dirichlet distribution, which specifies a distribution of topics within documents\n",
    "* $\\lambda_{w_{d,n}}$: the parameter of Dirichlet distribution, which specifies a distribution of words within a topic\n",
    "\n",
    "The first part of the equation estimates how probable is topic $k$ in document $d$ and the second part estimates how probable is occurence of word $w_{d,n}$ in topic $k$. Lastly, the distribution of all the topics is normalized and a new topic is sampled for a given word.\n",
    "\n",
    "\n",
    "If you want to dive deep into LDA and more thorough explanation of the topic models, I recommend watching a two-parts lecture of prof. David Blei, the co-creator of LDA available here: http://videolectures.net/mlss09uk_blei_tm/\n",
    "\n",
    "Now, let's prepare some Wikipedia articles to test LDA on them. 5 groups of relatively distinct \"_topics_\" were selected: biographies of actors, writers, painters, football players and politicians. To ensure even representation of all of those professions, 5000 articles were sampled for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice as sample, seed\n",
    "\n",
    "def extract_topic_articles(list_of_art, words = ['aktor']):\n",
    "    all_articles = []\n",
    "    tmp = []\n",
    "    # check if given word(s) occur in the first 500 characters\n",
    "    for idx, art in enumerate(list_of_art):\n",
    "        if any(x in art['text'][:500] for x in words):\n",
    "            tmp.append((art['title'], art['text']))\n",
    "    \n",
    "    # removing words that will be overrepresented from definition\n",
    "    for word in words:\n",
    "        tmp = [(x[0], x[1].replace(word, \"\")) for x in tmp]\n",
    "    all_articles.extend(tmp)\n",
    "    return(all_articles)\n",
    "\n",
    "\n",
    "# take 5k articles from each of the professions\n",
    "actors = extract_topic_articles(biogr, ['aktorka', 'aktor'])[:5000]\n",
    "politicians = extract_topic_articles(biogr, ['polityk'])[:5000]\n",
    "painters = extract_topic_articles(biogr, ['malarz', 'malarka'])[:5000]\n",
    "football = extract_topic_articles(biogr, ['piłkarz', 'piłkarka'])[:5000]\n",
    "writers = extract_topic_articles(biogr, ['pisarz', 'pisarka'])[:5000]\n",
    "\n",
    "art_lda = actors + politicians + painters + football + writers\n",
    "art_lda_txt = [x[1] for x in art_lda]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is ready, now it's time to create a matrix with counts of words in each of the documents. One could use tf (or tf-idf) statistics as a martix' values as well. The object called $\\text{tf_matrix}$ is a sparse matrix of shape $25000 x (\\text{number of unique words in a vocabulary})$. One can notice that the most important terms in 2 out of 5 topics (topic 1 and 3 to be precise) concern football. The fourth topic may be related to politicians and the fifth to art-related articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "TOPIC 1: \n",
      " występować mecz klub grać pozycja reprezentacja kariera zadebiutować lato zdobyć piłkarski sezon wystąpić zespół zawodnik\n",
      "TOPIC 2: \n",
      " de francuski la włoski hiszpański josé meksykański san szwedzki paryżu primera hiszpanii jean di división\n",
      "TOPIC 3: \n",
      " latach min później zadebiutował meczu sierpnia zdobył pozycji reprezentacji stycznia karierę 2010 marca lipca warszawie\n",
      "TOPIC 4: \n",
      " polski lato członek partia rok uniwersytet następnie pracować funkcja minister stanowisko wojna ukończyć narodowy warszawa\n",
      "TOPIC 5: \n",
      " sztuk rok powieść książka lato życie polski mina obraz nagroda artysta zostać film praca the\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer()\n",
    "tf_matrix = tf_vectorizer.fit_transform(art_lda_txt)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Latent Dirichlet Allocation with 5 hidden topics\n",
    "# As LDA is stochastic in its nature, you may get different results\n",
    "lda_obj = LatentDirichletAllocation(n_components=5, verbose=1)\n",
    "lda_obj.fit(tf_matrix)\n",
    "# prediction:\n",
    "lda_scores = lda_obj.transform(tf_matrix)\n",
    "\n",
    "\n",
    "# 15 most important words for each of latent topic:\n",
    "important_features = []\n",
    "for topic_idx, topic in enumerate(lda_obj.components_):\n",
    "    top = \" \".join([tf_feature_names[i] for i in topic.argsort()[:-15 - 1:-1]])\n",
    "    important_features.append(top)\n",
    "\n",
    "for idx, x in enumerate(important_features):\n",
    "    print(f\"TOPIC {idx+1}: \\n {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 1 topic: 5\n",
      "doc: 2 topic: 4\n",
      "doc: 3 topic: 1\n",
      "doc: 4 topic: 5\n",
      "doc: 5 topic: 5\n"
     ]
    }
   ],
   "source": [
    "# to get the most important topic for the first 5 articles just type:\n",
    "for n in range(lda_scores.shape[0])[:5]:\n",
    "    topic_most_pr = lda_scores[n].argmax()\n",
    "    print(\"doc: {} topic: {}\".format(n+1,topic_most_pr+1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
